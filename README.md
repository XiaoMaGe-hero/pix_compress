## 深度隐因子压缩（DLFC）

我们认为，限制SVD图像压缩的关键在于它使用矩阵的运算规则， 即：
$$
P_{m,n} = M_{m,1}N_{1,n}, \;\;\;      p_{i,j} =M_{i,1} \times N_{1, j}
$$
在使用乘法作为元素间的运算时， SVD能分解地维度受到矩阵奇异值数量的影响， 为了达到期望的压缩率，必须选择损失精度，减少奇异值数量。我们希望通过深度学习的方式为它习得一套新的运算规则，这套运算规则具有更丰富的映射内容，能够让矩阵稳定的分解为期望的尺度：
$$
P_{m,n} = M_{m,1} \times N_{1,n}, \;\;\; p_{i,j} = f(M_{i,1}, N_{1,j})
$$
如对于矩阵 $P_{m,n}$, 我们期望压缩至两个二维的向量：
$$
P_{m,n} = M_{m,2} \times N_{2, n},\;\;\;\; p_{i,j} = f(M_{i,1}, M_{i_2}, N_{1,j}, N_{2, j})
$$
 我们知道任意矩阵$P$都能通过LR基于无限种可能分解成 $M_{m,r} \times N_{r,n}$，这又有无限种可能使得：
$$
M_{m,r} \times L_{r,2} = M_{m,2} \;\;\; N_{r,n} \times H_{2,r} = N_{2,n}
$$
当给定我们矩阵 $P$, 虽然上述矩阵运算都是不可逆的，即不能直接求解积的因子，但是基于解的多样性我们为什么不能得到两个分局矩阵和相应的运算规则呢？当然可以。由于我们拟合的目标是新的运算规则 $f$， 所以只要我们的深度模型表达能力足够，对于所有样本空间的数据，都能够用一个训练好模型实现他们的分解，即找到分解矩阵 $M,N$。这一点与SVD思路相同，可以通过梯度下降，基于拟合出的运算规则 $f$， 求解分解矩阵。

网络示例(可行的网络之一， 看， 非常简单！)：

```python
    net = torch.nn.Sequential(
        torch.nn.Linear(size_dim, size_dim),
        torch.nn.Linear(size_dim, 2),
        # 预测的结果
        torch.nn.Linear(2, 100),
        torch.nn.ReLU(),
        torch.nn.Linear(100, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1),
       # torch.nn.Sigmoid(),
    )
```



Conclusion： 不同于端到端的图片压缩，我们使用深度学习的方法拟合向量元素的组合关系到矩阵元素的映射关系，这将得到一个非常轻量的网络（如：两个输入一个输出），并且它有非常好的并行计算能力，结合矩阵分解的架构，保证了压缩速度。我们并未在任何文献或是网络分享中看到过这一想法或是类似想法，声明对它的原创性。

​	当然它目前还处于研究状态，关于网络的训练、参数更新等等还有很多地方可优化。 我希望我的最终结果是由它产生，遗憾的是，思考这些方法就占用了我很多时间:crying_cat_face:。希望能有机会把它做得更好，能够应用到实际中去，那该是很有趣的事情！:smile_cat:
